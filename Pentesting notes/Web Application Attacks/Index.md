* [[Methodology]]
* Enumeration using Nmap: `sudo nmap -sV -p<port> --script= http* <ip>`
* `nikto` to perform scan on website.
	* `nikto -h <host> -Tuning b`: Runs software identification modules using nikto.
* Technology Stack Identification with [Wappalyzer](https://www.wappalyzer.com/) or kali linux tool `whatweb`: Can be used to analyze the tech stack of the web application.
* `wafw00f`: Command-line tool specifically designed for identifying Web Application Firewalls (WAFs).
* If the website uses wordpress use `wpscan` to scan wordpress application and plugins on website.
* [[Directory busting and fuzzing]]
* Security Testing with [Burp Suite](https://portswigger.net/burp): 
	* [Proxy](https://portswigger.net/burp/documentation/desktop/settings/tools/proxy): Set up proxy for an [external browser](https://portswigger.net/burp/documentation/desktop/external-browser-config).
	* [Repeater](https://portswigger.net/burp/documentation/desktop/tools/repeater): Craft new requests or easily modify the ones in History, resend them, and review the responses
	* [Intruder](https://portswigger.net/burp/documentation/desktop/tools/intruder): Automated attacking from various angles.
* Debugging Web Content: Use [inspector tool](https://firefox-source-docs.mozilla.org/devtools-user/tips/index.html) in a browser to inspect page content, scripts, files.
* Inspecting HTTP Response Headers: Use the [network tab](https://firefox-source-docs.mozilla.org/devtools-user/network_monitor/) in browser developer tools to view request and response headers  
* Sitemaps and robots.txt and other meta files:
	* [robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro): Mechanism that disallows search engine crawlers to index a page. Used to disallow any sensitive information or overload site with request for a page.
	* [Sitemaps](https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview#:~:text=A%20sitemap%20is%20a%20file,crawl%20your%20site%20more%20efficiently.): A website can highlight important information about pages on websites. Information is used by search engine crawlers leading to search engine optimization.
	* Other meta files such as Changelog, CHANGELOG, security.txt, humans.txt etc can reveal more information as well.
	* Check if `phpinfo()` file is exposes as something like phpinfo.php file if php is used in the web application.
	* [.well-known](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml): Check for .well-known files which are usually in `/.well-known/` directory of a website.
* Web crawling: 
	* web crawlers using python `scrapy` library:
		* Using hack the box academy scrapy bot: `wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip`
* API enumeration: 
	* Using gobuster: gobuster dir -u \<url\> -w \<wordlist file\> -p \<pattern\>
		* wordlist to try: /usr/share/wordlists/dirb/big.txt
		* pattern file example: 
		  {GOBUSTER}/v1
	         {GOBUSTER}/v2
		* Use curl in conjunction with gobuster to craft and test requests. Make sure to also print headers the response.
	* Use burpsuite intruder and repeater modules. Repeater for testing various requests. Intruder for using wordlists to enumerate API methods.
* Web reconnaissance frameworks: `Final Recon, Recon-ng, theHarvester, SpiderFoot, OSINT Framework`
* Default credentials: When you encounter a service with login, first see if it can be cracked with default credentials.
	* If needed use Seclists's default credentials wordlist manually or use Hydra from "Online password cracking" section.
	* Check techniques from [Password Cracking](obsidian://open?vault=Penetration%20testing&file=PEN200%2FPassword%20Attacks%2FPassword%20Cracking) 
* Check for [[Common web attacks]]
